{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import uuid\n",
    "import cloudpickle\n",
    "import faiss\n",
    "\n",
    "import chromadb\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.query_constructor.base import load_query_constructor_chain\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"../data/embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_key():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "    api_key = config['DEFAULT']['OpenAI_API_Key']\n",
    "    return api_key\n",
    "\n",
    "def initialize_openai_embeddings():\n",
    "    # Load the OpenAI embeddings\n",
    "    api_key = get_openai_key()\n",
    "    openai_embeddings = OpenAIEmbeddings(api_key=api_key, model=\"text-embedding-3-large\")\n",
    "\n",
    "    # Define a custom Embeddings class\n",
    "    class CustomEmbeddings(Embeddings):\n",
    "        openai_embeddings: OpenAIEmbeddings  # Define the openai_embeddings field\n",
    "\n",
    "        def __init__(self, openai_embeddings):\n",
    "            self.openai_embeddings = openai_embeddings\n",
    "\n",
    "        def embed_documents(self, texts):\n",
    "            return self.openai_embeddings.embed_documents(texts)\n",
    "\n",
    "        def embed_query(self, text):\n",
    "            return self.openai_embeddings.embed_query(text)\n",
    "\n",
    "    # Create an instance of the custom Embeddings class\n",
    "    embeddings = CustomEmbeddings(openai_embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def save_database_components(db, DB_PATH):\n",
    "    \"\"\"Save the components of the database\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.makedirs(DB_PATH)\n",
    "\n",
    "    # Create an empty FAISS index if it doesn't exist\n",
    "    if not hasattr(db, 'index'):\n",
    "        db.index = faiss.IndexFlatL2(1536)  # Assuming the embedding dimension is 1536 for text-embedding-3-large\n",
    "\n",
    "    # Save the faiss index\n",
    "    faiss.write_index(db.index, os.path.join(DB_PATH, \"faiss.index\"))\n",
    "\n",
    "    # Create an empty docstore if it doesn't exist\n",
    "    if not hasattr(db, 'docstore'):\n",
    "        db.docstore = {}\n",
    "\n",
    "    # Save the docstore\n",
    "    with open(os.path.join(DB_PATH, \"docstore.pkl\"), \"wb\") as f:\n",
    "        cloudpickle.dump(db.docstore, f)\n",
    "\n",
    "    # Create an empty index_to_docstore_id if it doesn't exist\n",
    "    if not hasattr(db, 'index_to_docstore_id'):\n",
    "        db.index_to_docstore_id = {}\n",
    "\n",
    "    # Save the index_to_docstore_id\n",
    "    with open(os.path.join(DB_PATH, \"index_to_docstore_id.pkl\"), \"wb\") as f:\n",
    "        cloudpickle.dump(db.index_to_docstore_id, f)\n",
    "\n",
    "    print(\"Database components saved successfully.\")\n",
    "\n",
    "\n",
    "def convert_to_list(doc, field):\n",
    "    \"\"\"Converts a comma-separated string from the specified field in a document to a list of strings.\"\"\"\n",
    "    # Check if the field exists in the document's metadata and it contains a string\n",
    "    if field in doc.metadata and isinstance(doc.metadata[field], str):\n",
    "        # Split the string by commas and strip any surrounding whitespace from each item\n",
    "        doc.metadata[field] = [item.strip() for item in doc.metadata[field].split(',')]\n",
    "    else:\n",
    "        # If the field doesn't exist or doesn't contain a string, do nothing\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probe_ID</th>\n",
       "      <th>Probe_Name</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Compatible_Systems</th>\n",
       "      <th>Probe_Type</th>\n",
       "      <th>Frequency_Range</th>\n",
       "      <th>Stock</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C3</td>\n",
       "      <td>ATL</td>\n",
       "      <td>HDI 5000</td>\n",
       "      <td>Curved Array</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>The ATL C3 is a convex curved array ultrasound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C4-2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>HDI 5000</td>\n",
       "      <td>Curved Array</td>\n",
       "      <td>2-4 MHz</td>\n",
       "      <td>2</td>\n",
       "      <td>The ATL C4-2 is a convex ultrasound transducer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C5-2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>HDI 1500, HDI 3000, HDI 3500, HDI 5000</td>\n",
       "      <td>Curved Array</td>\n",
       "      <td>2-5 MHz</td>\n",
       "      <td>7</td>\n",
       "      <td>The ATL C5-2 Curved Array transducer is a vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C5-IVT</td>\n",
       "      <td>ATL</td>\n",
       "      <td>UM9 HDI, HDI 1500, HDI 3000, HDI 3500, HDI 5000</td>\n",
       "      <td>Intracavitary</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>The ATL C5-IVT curved linear ultrasound transd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C7-4</td>\n",
       "      <td>ATL</td>\n",
       "      <td>UM9 HDI, HDI 1500, HDI 3000, HDI 5000</td>\n",
       "      <td>Curved Array</td>\n",
       "      <td>4-7 MHz</td>\n",
       "      <td>0</td>\n",
       "      <td>The ATL C7-4 curved linear ultrasound transduc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Probe_ID Probe_Name Manufacturer  \\\n",
       "0         1         C3          ATL   \n",
       "1         2       C4-2          ATL   \n",
       "2         3       C5-2          ATL   \n",
       "3         4     C5-IVT          ATL   \n",
       "4         5       C7-4          ATL   \n",
       "\n",
       "                                Compatible_Systems     Probe_Type  \\\n",
       "0                                         HDI 5000   Curved Array   \n",
       "1                                         HDI 5000   Curved Array   \n",
       "2           HDI 1500, HDI 3000, HDI 3500, HDI 5000   Curved Array   \n",
       "3  UM9 HDI, HDI 1500, HDI 3000, HDI 3500, HDI 5000  Intracavitary   \n",
       "4            UM9 HDI, HDI 1500, HDI 3000, HDI 5000   Curved Array   \n",
       "\n",
       "  Frequency_Range  Stock                                        Description  \n",
       "0                      0  The ATL C3 is a convex curved array ultrasound...  \n",
       "1         2-4 MHz      2  The ATL C4-2 is a convex ultrasound transducer...  \n",
       "2         2-5 MHz      7  The ATL C5-2 Curved Array transducer is a vers...  \n",
       "3                      0  The ATL C5-IVT curved linear ultrasound transd...  \n",
       "4         4-7 MHz      0  The ATL C7-4 curved linear ultrasound transduc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the probes data\n",
    "probes_df = pd.read_csv('../data/probes.csv')\n",
    "probes_df.fillna(\"\", inplace=True)\n",
    "#probes_df['Description'].fillna(\"\", inplace=True)\n",
    "probes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database components saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the custom Embeddings class\n",
    "embeddings = initialize_openai_embeddings()\n",
    "\n",
    "# Create the DB_PATH directory if it doesn't exist\n",
    "DB_PATH = \"../data/embeddings/\"\n",
    "os.makedirs(DB_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize Chroma vector store with the custom Embeddings instance\n",
    "db = Chroma(persist_directory=\"../data/embeddings/chromaDB\", embedding_function=embeddings, collection_name = 'my_collection')\n",
    "\n",
    "def populate_vector_db(probes_df, db, DB_PATH):\n",
    "    \"\"\"Extract embedding content and metadata from dataframe and populates the vector database with documents\"\"\"\n",
    "    # Process each row in the DataFrame\n",
    "    for index, row in probes_df.iterrows():\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        # Metadata for filtering\n",
    "        metadata = {\n",
    "            \"Probe_ID\": row['Probe_ID'],\n",
    "            \"Probe_Name\": row['Probe_Name'],\n",
    "            \"Manufacturer\": row['Manufacturer'],\n",
    "            \"Compatible_Systems\": ', '.join(row['Compatible_Systems'].split(', ')),  # Convert list to a comma-separated string\n",
    "            \"Probe_Type\": row['Probe_Type'],\n",
    "            \"Frequency_Range\": row['Frequency_Range'],\n",
    "            \"Stock\": row['Stock']\n",
    "        }\n",
    "        \n",
    "        # Ensure page_content is a string, replace NaN with an empty string\n",
    "        page_content = str(row['Description']) if pd.notna(row['Description']) else \"\"\n",
    "        \n",
    "        # Skip the row if the Description column is empty\n",
    "        if not page_content:\n",
    "            continue\n",
    "        \n",
    "        # Split the content into smaller chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=256)\n",
    "        chunks = text_splitter.split_text(page_content)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            random_uuid = str(uuid.uuid4())  \n",
    "            texts.append(chunk)\n",
    "            \n",
    "            # Create a unique path for each chunk\n",
    "            chunk_file_path = f\"{DB_PATH}chunks/{random_uuid}.txt\"\n",
    "            os.makedirs(os.path.dirname(chunk_file_path), exist_ok=True)\n",
    "            with open(chunk_file_path, \"w\") as file:\n",
    "                file.write(chunk)\n",
    "            \n",
    "            metadatas.append({\n",
    "                'id': random_uuid,\n",
    "                'probe_description_path': page_content,\n",
    "                'chunk_file_path': chunk_file_path,\n",
    "                **metadata\n",
    "            })\n",
    "        \n",
    "        # Add the text chunks and their metadata to the database.\n",
    "        db.add_texts(texts, metadatas)\n",
    "\n",
    "    return db\n",
    "\n",
    "db = populate_vector_db(probes_df, db, DB_PATH)\n",
    "\n",
    "# Save components of the database\n",
    "save_database_components(db, DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_db(DB_PATH=\"../data/embeddings/\"):\n",
    "    # Initialize variables for the components of the database.\n",
    "    db = None\n",
    "    docstore = None\n",
    "    index_to_docstore_id = None\n",
    "\n",
    "    # Check if the database already exists. If it does, load its components.\n",
    "    if os.path.exists(DB_PATH):\n",
    "        with open(os.path.join(DB_PATH, \"docstore.pkl\"), \"rb\") as f:\n",
    "            docstore = cloudpickle.load(f)\n",
    "        with open(os.path.join(DB_PATH, \"index_to_docstore_id.pkl\"), \"rb\") as f:\n",
    "            index_to_docstore_id = cloudpickle.load(f)\n",
    "        index = faiss.read_index(os.path.join(DB_PATH, \"faiss.index\"))\n",
    "    else:\n",
    "        # If the database does not exist, create a new FAISS index.\n",
    "        index = faiss.IndexFlatL2(3072)\n",
    "\n",
    "    # Load the OpenAI embeddings\n",
    "    embeddings = initialize_openai_embeddings()\n",
    "\n",
    "    # Create the FAISS vector database with the loaded or new components.\n",
    "    db = FAISS(\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(docstore),\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "    return db\n",
    "\n",
    "vectorstore = load_vector_db(DB_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
